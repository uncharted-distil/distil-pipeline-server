syntax = "proto3";
option go_package = "pipeline";

import "core.proto";

/*
 * Data Operations API Messages (Extension)
 *
 * This interface provides Protobuf message definitions and GRPC calls to allow a TA3 system to request
 * in-memory data changes be made by a target TA2 system.
 */

message FeatureData {
    repeated Feature from_feature = 1; // feature source data
    string dataset_uri = 2;            // URI of datset to add feature to
}

message AddFeaturesRequest {
    SessionContext context = 1;
    repeated FeatureData features = 2;
}

message RemoveFeaturesRequest {
    SessionContext context = 1;
    repeated Feature features = 2;
}

message Sample {
    string sample_id = 1; // value from the d3mIndex column (see https://datadrivendiscovery.org/wiki/display/gov/Dataset+Annotation+Schema)
    string data_uri = 2;  // URI pointing to source data. Format follows that of train/target data from
                          // https://datadrivendiscovery.org/wiki/display/gov/Dataset+Annotation+Schema, row selected
                          // using sample_id value
}

message SampleData {
    repeated Sample from_sample = 1; // sample source data
    string dataset_uri = 2;          // URI of dataset to add samples to
}

message AddSamplesRequest {
    SessionContext context = 1;
    repeated SampleData samples = 2;
}
message RemoveSamplesRequest {
    SessionContext context = 1;
    repeated Sample sample_ids = 2;
}

message ReplacementData {
    string value = 1;          // value to add - type is assumed to match value being replaced
    string feature_id = 2;     // feature value belongs to
    string sample_id = 3;      // sample value belongs to
    string dataset_uri = 4;    // uri of dataset being modified
}

message ReplaceDataRequest {
    SessionContext context = 1;
    repeated ReplacementData replacements = 2;
}

message MaterializeRequest {
    SessionContext context = 1;
    string source_dataset_uri = 2; // uri of dataset being modified
    string dest_dataset_uri = 3;   // uri to persist to
}

message TrainValidationSplitRequest {
    SessionContext context = 1;
    float val_size = 2;     // value between 0 and 1 representing validation proportion
    int32 seed = 3;         // seed for deterministic split
    bool is_validation = 4; // filters to validation set if true, filters to train set if false
    string dataset_uri = 5; // uri of dataset being split
}

message RevertRequest {
    string dataset_uri = 1; // uri of dataset being reverted
}

service DataExt {
    // Add and remove features to/from datasets
    rpc AddFeatures(AddFeaturesRequest) returns (Response) {}
    rpc RemoveFeatures(RemoveFeaturesRequest) returns (Response) {}

    // Add and remove records (rows) to/from datasets
    rpc AddSamples(AddSamplesRequest) returns (Response) {}
    rpc RemoveSamples(RemoveSamplesRequest) returns (Response) {}

    // Replace individual data points in a set
    rpc ReplaceData(ReplaceDataRequest) returns (Response) {}

    // Persist the dataset with modifications applied for future use
    rpc Materialize(MaterializeRequest) returns (Response) {}

    // Deterministic split of a dataset into training a validation
    // Filters out all but validation records or training records depending on is_validation
    rpc TrainValidationSplit(TrainValidationSplitRequest) returns (Response) {}

    // Revert the dataset to the original state
    rpc Revert(RevertRequest) returns (Response) {}
}
